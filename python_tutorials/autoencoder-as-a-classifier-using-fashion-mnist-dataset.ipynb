{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder as a classifier using Fashion-MNIST dataset\n",
    "https://www.datacamp.com/community/tutorials/autoencoder-classifier-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #model will be trained on GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthias\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13819515250715151009\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3163645542\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14660920388307965757\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Read data\n",
    "- Define function that opens the gzip file and reads the file using bytestream.read()\n",
    "- Pass the image dimension\n",
    "- Pass the number of images you habe in total\n",
    "- Using np.frombuffr to convert string stored in variable bug into a NumPy array of type float32\n",
    "- Reshape array in to a three-dimensional array or tensor where the first dimension is a number of images, and the second and third dimension being the dimension of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, 28,28)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extract_data('data/train-images-idx3-ubyte.gz', 60000)\n",
    "test_data = extract_data('data/t10k-images-idx3-ubyte.gz', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Read labels\n",
    "-  Define an extract labels function that opens the gzip file, reads the file using bytestream.read()\n",
    "- Pass label dimension (1)\n",
    "- Pass number of images you have in total\n",
    "- Using no.frombuffer to convert string stored in variable bug into a NumPy array of type int64\n",
    "- No need to reshape the array since the variable labels will return a column vector of dimension 60,000 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = extract_labels('data/train-labels-idx1-ubyte.gz',60000)\n",
    "test_labels = extract_labels('data/t10k-labels-idx1-ubyte.gz',10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: (60000, 28, 28)\n",
      "Test set (images) shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=train_data.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'A',\n",
    " 1: 'B',\n",
    " 2: 'C',\n",
    " 3: 'D',\n",
    " 4: 'E',\n",
    " 5: 'F',\n",
    " 6: 'G',\n",
    " 7: 'H',\n",
    " 8: 'I',\n",
    " 9: 'J',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'(Label: E)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(train_data[10], (28,28))\n",
    "curr_lbl = train_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(test_data[10], (28,28))\n",
    "curr_lbl = test_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.reshape(-1, 28,28, 1)\n",
    "test_data = test_data.reshape(-1, 28,28, 1)\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtype, test_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255.0, 255.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_data), np.max(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data / np.max(train_data)\n",
    "test_data = test_data / np.max(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_data), np.max(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n",
    "                                                             train_data,\n",
    "                                                             test_size=0.2,\n",
    "                                                             random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 200\n",
    "inChannel = 1\n",
    "x, y = 28, 28\n",
    "input_img = Input(shape = (x, y, inChannel))\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    return conv4\n",
    "\n",
    "def decoder(conv4):    \n",
    "    #decoder\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 7, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 1,758,657\n",
      "Trainable params: 1,755,841\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 38s 795us/step - loss: 0.0196 - val_loss: 0.0135\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 35s 722us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 35s 721us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 35s 727us/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 34s 717us/step - loss: 0.0047 - val_loss: 0.0052\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 34s 718us/step - loss: 0.0041 - val_loss: 0.0063\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 35s 719us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 34s 718us/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 34s 719us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 34s 717us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 35s 719us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 35s 720us/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 34s 718us/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 34s 717us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 35s 722us/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 34s 719us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 17/200\n",
      "38720/48000 [=======================>......] - ETA: 6s - loss: 0.0022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a29505c4b436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mautoencoder_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ground\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_ground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
